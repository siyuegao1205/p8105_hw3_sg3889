---
title: "P8105 Homework 3"
author: "Siyue Gao"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
  fig.height = 6,
  fig.asp = .6,
  out.width = "90%"
)
```

```{r}
library(tidyverse)
library(p8105.datasets)
```


# Problem 1

## Load the data

```{r}
data("instacart")
```

## Data description

The `instacart` dataset contains `r nrow(instacart)` observations of `r n_distinct(instacart$user_id)` unique users from the "train" evaluation set, with `r ncol(instacart)` variables, primarily describing information about Instacart online order usage. Some key variables in the dataset are as follows:

*   `user_id`: user identifier
*   `product_id` and `product_name`: product identifier and name of the product, respectively
*   `aisle_id` and `aisle`: aisle identifier and name of the aisle, respectively
*   `department_id` and `department`: department identifier and name of the department, respectively
*   some variables related to the `order`, such as:
    *   `order_id`: order identifier
    *   `add_to_cart_order`: sequence of each product being added into cart in each order
    *   `order_number`: order sequence for each unique user
    *   `order_dow` and `order_hour_of_day`: time that the order was placed on
    *   `days_since_prior_order`: days since the last order

To give an illustrative example of the order NO.1 in this dataset:

```{r}
instacart %>% 
  filter(order_id == 1)
```

In the "train" evaluation set, the user 112108 ordered 8 products from dairy eggs, produce, and canned goods department in this order, including Bulgarian Yogurt, Organic 4% Milk Fat Whole Milk Cottage Cheese, Organic Celery Hearts, Cucumber Kirby, Lightly Smoked Sardines in Olive Oil, Bag of Organic Bananas, Organic Hass Avocado, and Organic Whole String Cheese. The user 112108 placed this order on Wednesday, 10 am, and this order is the 4th one for this user. The prior order of the user 112108 was placed 9 days ago.


## Some Questions Related to `instacart`

*   How many aisles are there, and which aisles are the most items ordered from?

```{r}
instacart %>% 
  group_by(aisle) %>% 
  summarise(
    order_count = n()
  ) %>% 
  arrange(desc(order_count)) %>% 
  head(2)
```

There are `r n_distinct(instacart$aisle)` unique aisles, and the most items are ordered from aisle "fresh vegetables", followed by aisle "fresh fruits".

*   Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

```{r}
instacart %>% 
  group_by(aisle) %>% 
  summarise(
    order_count = n()
  ) %>% 
  filter(order_count > 10000) %>% 
  mutate(
    aisle = fct_reorder(aisle, order_count)
  ) %>% 
  ggplot(aes(x = aisle, y = order_count)) +
  geom_col() +
  labs(
      x = "Aisle",
      y = "Number of Items Ordered in Each Aisle",
      title = "Frequency of Items Ordered in Each Aisle",
      caption = "Data are limited to aisles with more than 10,000 items ordered."
    ) +
  coord_flip() +
  theme_bw() +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
    )
```

The plot above shows the number of items ordered in each aisle, with data limited to aisles with more than 10000 items ordered. We can tell that the most items are ordered from aisle "fresh vegetables", followed by aisle "fresh fruits".

*   Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

```{r}
instacart %>% 
  filter(
    aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")
    ) %>% 
  group_by(product_name) %>% 
  mutate(
    times = n()
  ) %>% 
  relocate(aisle) %>% 
  arrange(aisle, desc(times)) %>% 
  distinct(aisle, product_name, times) %>%
  group_by(aisle) %>% 
  top_n(n = 3)
```

The table above shows the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

Of the aisle "baking ingredients", the most popular item is Light Brown Sugar, followed by Pure Baking Soda and Cane Sugar. Of the aisle "dog food care", the most popular item is Snack Sticks Chicken & Rice Recipe Dog Treats, followed by Organix Chicken & Brown Rice Recipe and Small Dog Biscuits. Of the aisle "packaged vegetables fruits", the most popular item is Organic Baby Spinach, followed by Organic Raspberries and Organic Blueberries. 

*   Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}
instacart %>% 
  filter(
    product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")
    ) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(
    mean_hour = mean(order_hour_of_day, na.rm = TRUE)
  ) %>% 
  mutate(
    order_dow = order_dow + 1,
    order_dow = lubridate::wday(order_dow, label = TRUE, abbr = FALSE)
  ) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```

The table above shows the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. To give an illustrative example, the mean hour of Pink Lady Apples being ordered on Sunday is 13.4.


# Problem 2

## Load, tidy, and wrangle the data

The following code chunk is to tidy the data by `pivot_longer()`, create a new variable called `weekday_vs_weekend` to indicate whether the day is weekday or weekend, and finally encode some variables (`day` and `minute`) into more reasonable classes. The resulting tidied dataset is arranged by `week` and `day`.

```{r}
accelerometer = read_csv("data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    values_to = "activity_counts",
    names_prefix = "activity_"
  ) %>% 
  mutate(
    day = factor(day, 
                 levels = c("Sunday", "Monday", "Tuesday", "Wednesday",
                            "Thursday", "Friday", "Saturday")),
    weekday_vs_weekend = ifelse(day %in%  c("Saturday", "Sunday"), "weekend", "weekday"),
    minute = as.numeric(minute)
  ) %>% 
  arrange(week, day)
```

The resulting dataset includes `r nrow(accelerometer)` observations with `r ncol(accelerometer)` variables, primarily recording some activity information of a 63 year-old male with BMI 25, collected by the accelerometer. Some key variables are `activity_counts`, indicating the activity counts for each minute; `week`, `day`, `minute`, showing the time of which the data point is collected; and `weekday_vs_weekend` is a binary indicator of either weekday or weekend.

## Traditional analyses of `accelerometer`

The following code chunk creates a new variable `total_counts` by aggregating across minutes to identify the total activity for each day. The table below shows the total activities by `week` and `day`.

```{r}
accelerometer = accelerometer %>% 
  group_by(day_id) %>% 
  mutate(
    total_counts = sum(activity_counts, na.rm = TRUE)
  )

accelerometer %>% 
  distinct(week, day, total_counts) %>% 
  print.data.frame()
```

To examine any apparent trends of total activity, we'd like to make a scatterplot with a trend line added.

```{r}
accelerometer %>% 
  ggplot(aes(x = day_id, y = total_counts, color = weekday_vs_weekend)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(
    x = "Day ID",
    y = "Total Activity Counts",
    title = "Total Activity Counts by Day ID",
    caption = "Day ID is the originally assigned ID by the accelerometer."    
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank()
  )
```

As the above figure shows, the total activity counts on weekdays seem to be more stable, while there are more fluctuations (larger variances) on weekends. However, we should note that there are several days with total activity counts as low as 1440 (with every minute only counting "1"), indicating that the accelerometer might fail to accurately record the data.

## Inspection activity over the course of the day

The following code chunk makes a single-panel plot showing the 24-hour activity time courses for each day.

```{r}
accelerometer %>%
  mutate(
    hour = minute / 60
  ) %>% 
  ggplot(aes(x = hour, y = activity_counts, color = day)) +
  geom_smooth(se = FALSE) +
  labs(
    x = "Hour of Day",
    y = "Activity Counts",
    title = "24-hour Activity Counts for Each Day",
    color = "Day of Week"
  ) +
  scale_x_continuous(
    breaks = c(0, 3, 6, 9, 12, 15, 18, 21, 24)
  ) +
  theme_minimal()
```

The plot above shows the 24-hour activity counts by hour for each day, and the different colors are used to indicate day of the week. We can tell that at around Sunday noon and on Friday late evening, the activity counts of this participant reach to two peaks, compared to other time courses. The activity counts tend to experience an obvious decline after about 10 pm each day, and there are always periods with lower activity counts from 0 to 5 am.


# Problem 3

## Load and clean the data

Load the data, separate `date` into `year`, `month`, and `day`. Change the units of temperature, precipitation, and snowfall into more reasonable and readable ones.

```{r}
data("ny_noaa")

ny_noaa = ny_noaa %>%
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(
    year = as.numeric(year),
    month = as.numeric(month),
    day = as.numeric(day),
    prcp = ifelse(!is.na(prcp), prcp / 10, NA_real_),
    tmax = ifelse(!is.na(tmax), as.numeric(tmax) / 10, NA_real_),
    tmin = ifelse(!is.na(tmin), as.numeric(tmin) / 10, NA_real_)
  )
```

## Data description

```{r, eval = FALSE}
skimr::skim(ny_noaa)
```

The `ny_noaa` dataset contains `r nrow(ny_noaa)` observations of `r n_distinct(ny_noaa$id)` unique weather station in New York State, with `r ncol(ny_noaa)` variables, primarily describing climate information collected at given specific observation dates from `r min(ny_noaa$year)` to `r max(ny_noaa$year)`. Some key variables are recorded in the dataset, such as precipitation (mm), snowfall (mm), snow depth (mm), maximum and minimum temperature (degrees C). There are `r sum(is.na(ny_noaa$prcp))` NAs for precipitation, `r sum(is.na(ny_noaa$snow))` NAs for snowfall, `r sum(is.na(ny_noaa$snwd))` NAs for snow depth, `r sum(is.na(ny_noaa$tmax))` NAs for maximum temperature, and `r sum(is.na(ny_noaa$tmin))` NAs for minimum temperature. These missing values are due to the fact that each weather station may collect only a subset of these variables.

```{r}
ny_noaa %>% 
  group_by(snow) %>% 
  summarise(
    n_obs = n()
  ) %>% 
  mutate(
    rank = min_rank(desc(n_obs))
  ) %>% 
  arrange(rank)
```

For snowfall, the most commonly observed value is 0. It's reasonable since the snow usually occurs during winter months, which only take up 1/4 time of a whole year.

## Two-panel plot for average `tmax`

```{r}
ny_noaa %>% 
  filter(month %in% c(1, 7)) %>% 
  mutate(
    month = month.name[month]
  ) %>% 
  group_by(id, year, month) %>% 
  summarise(
    mean_tmax = mean(tmax, na.rm = TRUE)
  ) %>% 
  ggplot(aes(x = year, y = mean_tmax)) +
  geom_point(alpha = .5) +
  geom_smooth(color = "red", se = FALSE) +
  labs(
    x = "Year",
    y = "Average Maximum Temperature\n(degrees C)",
    title = "Average Maximum Temperature in Each Weather Station across Years",
    caption = "Data are limited to January and July.\nEach spot represent a unique weather station."
  ) +
  scale_x_continuous(
    breaks = c(1980, 1985, 1990, 1995, 2000, 2005, 2010)
  ) +
  facet_grid(. ~ month) +
  theme_minimal() +
  theme(
    panel.spacing = unit(1, "lines")
  )
```

For average maximum temperature in **January**, there is a continuous increase from 1981 to 1990. However, after 1990, it tends to have a regular fluctuation at around 0 Celcius degree, with a decrease lasting for 3 to 4 years followed by a continuous increase for approximately 4 to 5 years. There are some obvious outliers just by eyeballing, for instance, in 1982 and 2005, we can observe some stations with extremely lower mean maximum temperature (and also some in 2008 and 2010), while in 2004, there are some higher observations.

For average maximum temperature in **July**, the overall trend is more stable. The average maximum temperature is at around 27 celcius degrees across years. There are some obvious outliers, for instance, in 1989, we can observe a weather station with an extremely lower average maximum temperature, and also some lower observations in years such as 1981 to 1984, 2005, and 2007.

